{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a92a52-5c5b-4ed0-b2a8-5747b5fb0f55",
   "metadata": {},
   "source": [
    "### Week 7 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77defa95-f19d-402d-9f63-2e47c8ac1f9a",
   "metadata": {},
   "source": [
    "### Chicago Airbnb Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923e1585-0c8c-48c8-8475-599be395300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For handling data in DataFrames\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import seaborn as sns  # For advanced statistical visualizations\n",
    "import psycopg2  # For connecting to PostgreSQL databases\n",
    "\n",
    "# Scikit-learn modules for machine learning and preprocessing\n",
    "from sklearn.model_selection import train_test_split  # Splitting data into training and testing sets\n",
    "from sklearn.linear_model import LinearRegression  # Linear regression model\n",
    "from sklearn.metrics import mean_absolute_error, r2_score  # Model evaluation metrics\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Data normalization and encoding categorical variables\n",
    "from sklearn.impute import SimpleImputer  # Handling missing values\n",
    "\n",
    "# SQLAlchemy for database connection and execution\n",
    "from sqlalchemy import create_engine  # For managing database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e671670-890e-4536-b343-9fe8a9f6fead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully stored in PostgreSQL under the 'raw' schema.\n"
     ]
    }
   ],
   "source": [
    "# Load and Sample Data  \n",
    "df = pd.read_csv(\"listings.csv\")  # Load dataset  \n",
    "\n",
    "# Select a random sample of 100 rows  \n",
    "data = df.sample(n=100, random_state=42)  # Ensures reproducibility  \n",
    "\n",
    "# Save the sampled data to a new CSV file  \n",
    "data.to_csv(\"listings_data.csv\", index=False)  \n",
    "\n",
    "# Remove the sampled rows from the original DataFrame  \n",
    "df_remaining = df.drop(data.index)  \n",
    "\n",
    "# ### Database Connection (Supabase PostgreSQL)  \n",
    "DATABASE_URL = \"postgresql://postgres:fwSMslYZJjGfDstk@db.ahrshioxhhqpfnuwvufg.supabase.co:5432/postgres\"  \n",
    "engine = create_engine(DATABASE_URL)  \n",
    "\n",
    "# Store the remaining data in PostgreSQL under the \"raw\" schema  \n",
    "df_remaining.to_sql(\"listingstable\", engine, schema=\"raw\", if_exists=\"replace\", index=False)  \n",
    "\n",
    "print(\"Data successfully stored in PostgreSQL under the 'raw' schema.\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a513cd-a1b6-4865-8690-00c2d1b71655",
   "metadata": {},
   "source": [
    "# **Data Definition and Analytical Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62fca2-8490-48b2-af85-c887964e6c2f",
   "metadata": {},
   "source": [
    "# Dataset Features  \n",
    "\n",
    "- **id**: Unique identifier for each listing *(Nominal)*  \n",
    "- **name**: Listing name *(Nominal)*  \n",
    "- **host_id**: Unique identifier for the host *(Nominal)*  \n",
    "- **host_name**: Host's name *(Nominal)*  \n",
    "- **neighbourhood_group**: Broad geographical area *(Nominal)*  \n",
    "- **neighbourhood**: Specific neighborhood within the group *(Nominal)*  \n",
    "- **latitude**: Latitude coordinate *(Continuous)*  \n",
    "- **longitude**: Longitude coordinate *(Continuous)*  \n",
    "- **room_type**: Type of accommodation offered *(Nominal)*  \n",
    "- **price**: Nightly rate *(Continuous)*  \n",
    "- **minimum_nights**: Minimum required stay *(Discrete)*  \n",
    "- **number_of_reviews**: Total number of reviews received *(Discrete)*  \n",
    "- **last_review**: Date of the most recent review *(Nominal)*  \n",
    "- **reviews_per_month**: Average number of reviews per month *(Continuous)*  \n",
    "- **calculated_host_listings_count**: Total number of listings per host *(Discrete)*  \n",
    "- **availability_365**: Number of days the listing is available in a year *(Discrete)*  \n",
    "\n",
    "---\n",
    "\n",
    "# Analytical Inquiry  \n",
    "\n",
    "**\"What is the relationship between room type, neighborhood, and pricing in influencing the number of reviews a listing receives?\"**  \n",
    "\n",
    "**Target Variable**: *number_of_reviews*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e9646-ecf0-4ad3-85e0-6054b19ce3d6",
   "metadata": {},
   "source": [
    "# **Feature Selection**  \n",
    "\n",
    "For our analytical question, **\"How do room type, neighborhood, and pricing impact the number of reviews a listing receives?\"**, we select features that directly influence the target variable: **number_of_reviews**.  \n",
    "\n",
    "### **Selected Features:**  \n",
    "- **neighbourhood_group** – Broad geographical classification affecting listing popularity.  \n",
    "- **neighbourhood** – More specific location data, influencing pricing and demand.  \n",
    "- **room_type** – Accommodation type (e.g., private room, entire home), impacting customer preference.  \n",
    "- **price** – Cost per night, which may affect reviews and occupancy.  \n",
    "- **minimum_nights** – Booking restrictions that could influence total stays and reviews.  \n",
    "- **reviews_per_month** – Indicates how frequently guests leave feedback.  \n",
    "- **availability_365** – The number of days a listing is available, influencing the likelihood of receiving reviews.  \n",
    "\n",
    "### **Excluded Features:**  \n",
    "- **id, host_id** – Unique identifiers that don’t contribute to predictions.  \n",
    "- **name, host_name** – Free text fields with minimal predictive value.  \n",
    "- **last_review** – A date field, better represented by *reviews_per_month*.  \n",
    "- **calculated_host_listings_count** – More relevant for host analytics than guest review behavior.  \n",
    "\n",
    "---\n",
    "\n",
    "# **Data Cleaning and Preparation**  \n",
    "\n",
    "To ensure high-quality data for analysis and machine learning, we perform the following steps:  \n",
    "\n",
    "## **1. Handling Missing Values**  \n",
    "- Identify and address missing values.  \n",
    "- **Numerical features** – Use mean or median imputation.  \n",
    "- **Categorical features** – Apply mode imputation or remove records with excessive missing data.  \n",
    "\n",
    "## **2. Handling Outliers**  \n",
    "- Detect anomalies in numerical data using **boxplots or Z-scores**.  \n",
    "- Apply transformations such as **log scaling** or **capping extreme values** where necessary.  \n",
    "\n",
    "## **3. Encoding Categorical Variables**  \n",
    "- Convert non-numerical data into numerical form using:  \n",
    "  - **One-hot encoding** for nominal categories.  \n",
    "  - **Label encoding** for ordinal categories (if applicable).  \n",
    "\n",
    "## **4. Normalization & Scaling**  \n",
    "- Apply **Min-Max Scaling** or **Standardization** to ensure numerical features are on a comparable scale.  \n",
    "\n",
    "By implementing these steps, we refine the dataset for effective machine learning modeling.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d9a9f4-d063-4f03-960c-75343365af00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                  0\n",
       "name                                0\n",
       "host_id                             0\n",
       "host_name                           0\n",
       "neighbourhood_group               100\n",
       "neighbourhood                       0\n",
       "latitude                            0\n",
       "longitude                           0\n",
       "room_type                           0\n",
       "price                               0\n",
       "minimum_nights                      0\n",
       "number_of_reviews                   0\n",
       "last_review                        18\n",
       "reviews_per_month                  18\n",
       "calculated_host_listings_count      0\n",
       "availability_365                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd6fe7b-30cf-4dce-9f7d-797cd1b0b84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Feature Selection ---\n",
    "selected_features = [\n",
    "    \"neighbourhood_group\", \"neighbourhood\", \"room_type\", \"price\",\n",
    "    \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\", \"availability_365\"\n",
    "]\n",
    "\n",
    "# Create a copy of the selected data to prevent modifications to the original dataset\n",
    "df = data[selected_features].copy()\n",
    "\n",
    "# --- Step 2: Handling Missing Values ---\n",
    "\n",
    "# Drop 'neighbourhood_group' if it is entirely missing\n",
    "if df[\"neighbourhood_group\"].isna().all():\n",
    "    df.drop(columns=[\"neighbourhood_group\"], inplace=True)\n",
    "else:\n",
    "    # Fill missing categorical values with the most frequent category\n",
    "    df[\"neighbourhood_group\"] = SimpleImputer(strategy=\"most_frequent\").fit_transform(df[[\"neighbourhood_group\"]])\n",
    "\n",
    "# Fill missing values in numerical columns using the median\n",
    "df[\"reviews_per_month\"] = SimpleImputer(strategy=\"median\").fit_transform(df[[\"reviews_per_month\"]])\n",
    "\n",
    "print(\"Data preprocessing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96994b71-fe2f-4fc5-a2d6-20804ab47acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed successfully!\n",
      "      price  minimum_nights  number_of_reviews  reviews_per_month  \\\n",
      "0  1.168854       -0.576693          -0.692832          -1.235100   \n",
      "1 -0.922748       -0.576693          -0.712220          -0.161991   \n",
      "2 -0.909340       -0.576693          -0.692832          -0.421995   \n",
      "3  1.906278       -0.423363          -0.498942          -0.336903   \n",
      "4 -0.775263       -0.500028           1.672619          -0.024897   \n",
      "\n",
      "   availability_365  neighbourhood_Armour Square  neighbourhood_Avondale  \\\n",
      "0          1.432073                          0.0                     0.0   \n",
      "1         -1.278977                          0.0                     0.0   \n",
      "2          1.372490                          0.0                     0.0   \n",
      "3          1.260771                          0.0                     0.0   \n",
      "4          1.424625                          0.0                     0.0   \n",
      "\n",
      "   neighbourhood_Brighton Park  neighbourhood_Clearing  \\\n",
      "0                          0.0                     0.0   \n",
      "1                          0.0                     0.0   \n",
      "2                          0.0                     0.0   \n",
      "3                          0.0                     0.0   \n",
      "4                          0.0                     0.0   \n",
      "\n",
      "   neighbourhood_East Garfield Park  ...  neighbourhood_Rogers Park  \\\n",
      "0                               0.0  ...                        0.0   \n",
      "1                               0.0  ...                        0.0   \n",
      "2                               0.0  ...                        0.0   \n",
      "3                               0.0  ...                        0.0   \n",
      "4                               0.0  ...                        0.0   \n",
      "\n",
      "   neighbourhood_South Chicago  neighbourhood_South Lawndale  \\\n",
      "0                          0.0                           0.0   \n",
      "1                          0.0                           0.0   \n",
      "2                          1.0                           0.0   \n",
      "3                          0.0                           0.0   \n",
      "4                          0.0                           0.0   \n",
      "\n",
      "   neighbourhood_South Shore  neighbourhood_Uptown  neighbourhood_West Ridge  \\\n",
      "0                        0.0                   0.0                       0.0   \n",
      "1                        0.0                   0.0                       0.0   \n",
      "2                        0.0                   0.0                       0.0   \n",
      "3                        0.0                   0.0                       0.0   \n",
      "4                        0.0                   0.0                       0.0   \n",
      "\n",
      "   neighbourhood_West Town  neighbourhood_Woodlawn  room_type_Hotel room  \\\n",
      "0                      0.0                     0.0                   0.0   \n",
      "1                      0.0                     0.0                   0.0   \n",
      "2                      0.0                     0.0                   0.0   \n",
      "3                      0.0                     0.0                   0.0   \n",
      "4                      0.0                     0.0                   0.0   \n",
      "\n",
      "   room_type_Private room  \n",
      "0                     0.0  \n",
      "1                     1.0  \n",
      "2                     1.0  \n",
      "3                     0.0  \n",
      "4                     1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Feature Selection ---\n",
    "selected_features = [\n",
    "    \"neighbourhood_group\", \"neighbourhood\", \"room_type\", \"price\",\n",
    "    \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\", \"availability_365\"\n",
    "]\n",
    "\n",
    "df = data[selected_features].copy()\n",
    "\n",
    "# --- Step 2: Handling Missing Values ---\n",
    "\n",
    "# Drop 'neighbourhood_group' if it is completely missing\n",
    "if df[\"neighbourhood_group\"].isna().all():\n",
    "    df.drop(columns=[\"neighbourhood_group\"], inplace=True)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = [col for col in [\"neighbourhood_group\", \"neighbourhood\", \"room_type\"] if col in df.columns]\n",
    "numeric_features = [\"price\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\", \"availability_365\"]\n",
    "\n",
    "# Impute missing values for categorical columns\n",
    "if categorical_cols:\n",
    "    df[categorical_cols] = SimpleImputer(strategy=\"most_frequent\").fit_transform(df[categorical_cols])\n",
    "\n",
    "# Impute missing values for numerical columns\n",
    "df[numeric_features] = SimpleImputer(strategy=\"median\").fit_transform(df[numeric_features])\n",
    "\n",
    "# --- Step 3: Outlier Removal using Z-score ---\n",
    "def remove_outliers(data, columns, threshold=3):\n",
    "    \"\"\"Removes outliers based on Z-score.\"\"\"\n",
    "    for col in columns:\n",
    "        z_scores = np.abs((data[col] - data[col].mean()) / data[col].std())\n",
    "        data = data[z_scores < threshold]\n",
    "    return data\n",
    "\n",
    "df = remove_outliers(df, numeric_features)\n",
    "\n",
    "# --- Step 4: Encoding Categorical Features ---\n",
    "if categorical_cols:\n",
    "    encoder = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
    "    encoded_df = pd.DataFrame(encoder.fit_transform(df[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # Drop original categorical columns and merge encoded features\n",
    "    df.drop(columns=categorical_cols, inplace=True)\n",
    "    df = pd.concat([df.reset_index(drop=True), encoded_df], axis=1)\n",
    "\n",
    "# --- Step 5: Scaling Numerical Features ---\n",
    "df[numeric_features] = StandardScaler().fit_transform(df[numeric_features])\n",
    "\n",
    "# --- Save and Display Processed Data ---\n",
    "df.to_csv(\"cleaned_data.csv\", index=False)\n",
    "\n",
    "print(\"Data preprocessing completed successfully!\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3897960-08e0-472e-9788-cd2c7bbceca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a DataFrame to log data preprocessing actions\n",
    "cleansing_decisions = pd.DataFrame({\n",
    "    \"field_name\": [\"price\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\", \"availability_365\"],\n",
    "    \"manipulation_type\": [\"outlier_removal\", \"outlier_removal\", \"outlier_removal\", \"imputation\", \"outlier_removal\"],\n",
    "    \"threshold_or_value\": [\"Z-score < 3\", \"Z-score < 3\", \"Z-score < 3\", \"Median Imputation\", \"Z-score < 3\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38639a6c-8b66-4efa-8d19-f9ccb0b330ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created and data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Establish a connection to the PostgreSQL database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"MSDS610\",\n",
    "        user=\"postgres\",\n",
    "        password=\"your_new_password\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Define an SQL query to create the 'listingdata' table if it does not already exist\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS listingdata (\n",
    "        id SERIAL PRIMARY KEY,  -- Auto-incrementing primary key\n",
    "        field_name TEXT NOT NULL,  -- Column to store field names\n",
    "        manipulation_type TEXT NOT NULL,  -- Column to describe type of data manipulation\n",
    "        threshold_or_value TEXT  -- Column to store threshold values or specific replacement values\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the table creation query and commit the changes\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert data from the cleansing_decisions DataFrame into the table\n",
    "    for _, row in cleansing_decisions.iterrows():\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO listingdata (field_name, manipulation_type, threshold_or_value) VALUES (%s, %s, %s)\",\n",
    "            (row[\"field_name\"], row[\"manipulation_type\"], row[\"threshold_or_value\"])\n",
    "        )\n",
    "\n",
    "    # Commit the inserted data\n",
    "    conn.commit()\n",
    "    print(\"Table created and data inserted successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any exceptions and display an error message\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the database connection is closed properly\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ba53bd-a98b-450f-a1f7-9824df1ae0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Creating new variables to enhance data insights\n",
    "\n",
    "# Calculate price per night to normalize pricing across different listings\n",
    "df[\"price_per_night\"] = df[\"price\"] / df[\"minimum_nights\"]\n",
    "\n",
    "# Compute review rate as a measure of engagement (reviews per available day)\n",
    "df[\"review_rate\"] = df[\"number_of_reviews\"] / (1 + df[\"availability_365\"])\n",
    "\n",
    "# Identify luxury listings (top 10% based on price)\n",
    "luxury_threshold = df[\"price\"].quantile(0.90)\n",
    "df[\"is_luxury\"] = (df[\"price\"] > luxury_threshold).astype(int)\n",
    "\n",
    "# Store feature descriptions for documentation\n",
    "features_info = pd.DataFrame({\n",
    "    \"feature_name\": [\"price_per_night\", \"review_rate\", \"is_luxury\"],\n",
    "    \"description\": [\n",
    "        \"Computed as price divided by minimum nights to standardize cost impact\",\n",
    "        \"Ratio of number of reviews to availability (adjusted to avoid division by zero)\",\n",
    "        \"Binary classification for luxury listings (top 10% by price)\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Export feature details to a CSV file for reference\n",
    "features_info.to_csv(\"new_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b40a9d3-ddee-463e-978e-a43c0acefbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to scale\n",
    "features_to_normalize = [\"price_per_night\", \"review_rate\", \"price\", \"minimum_nights\", \"availability_365\"]\n",
    "scaler = StandardScaler()\n",
    "df[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72f4121-027d-42bc-9af1-8846fc6d7a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"MSDS610\",\n",
    "    user=\"postgres\",\n",
    "    password=\"your_new_password\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table if it doesn't exist\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS new_features (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        feature_name TEXT NOT NULL,\n",
    "        description TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "for _, row in features_info.iterrows():\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO new_features (feature_name, description) VALUES (%s, %s)\",\n",
    "        (row[\"feature_name\"], row[\"description\"])\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "print(\"Data inserted successfully!\")\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d835e16f-09dc-4f3a-b30e-0347896a57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable (what we want to predict) and feature variables (input data)\n",
    "X = df.drop(columns=[\"number_of_reviews\"])  # Features (all columns except the target)\n",
    "y = df[\"number_of_reviews\"]  # Target variable (the column we want to predict)\n",
    "\n",
    "# Split the dataset into training (70%), validation (15%), and testing (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42  # 30% of data reserved for validation and testing\n",
    ")\n",
    "\n",
    "# Further split the remaining 30% into validation (15%) and test (15%) sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42  # Splitting evenly into 15% validation and 15% test data\n",
    ")\n",
    "\n",
    "# Save validation data to CSV files for future model validation\n",
    "X_val.to_csv(\"X_val.csv\", index=False)  # Save validation features\n",
    "y_val.to_csv(\"y_val.csv\", index=False)  # Save validation target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fd47d-41dc-4088-8910-7447a039bc33",
   "metadata": {},
   "source": [
    "# **Choosing the Right Model for Prediction**  \n",
    "\n",
    "For our analysis, we require a **supervised learning algorithm** since we aim to predict a **continuous** target variable: **number_of_reviews**. Given that our dataset includes both categorical and continuous features, we explore regression models best suited for this task.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Why Use a Regression Model?**  \n",
    "- **Continuous Target Variable** – Since *number_of_reviews* is numeric, regression models are ideal.  \n",
    "- **Multivariable Impact Analysis** – We seek to understand how factors like *price, room type, and neighborhood* influence the number of reviews.  \n",
    "- **Baseline & Beyond** – A **linear regression model** provides a starting point, but we will explore **tree-based models** if the relationships are complex.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Candidate Models**  \n",
    "\n",
    "### **1. Linear Regression**  \n",
    "- Simple and interpretable, providing insights into feature impact.  \n",
    "- Limited for capturing complex patterns and non-linear relationships.  \n",
    "\n",
    "### **2. Random Forest Regressor**  \n",
    "- Effectively captures non-linearity and interactions between features.  \n",
    "- Provides feature importance, helping identify key influencers of review counts.  \n",
    "- More computationally intensive than linear models.  \n",
    "\n",
    "### **3. Gradient Boosting (XGBoost)**  \n",
    "- Optimized for better predictive accuracy through boosting.  \n",
    "- Handles missing values and outliers more effectively.  \n",
    "- Requires careful tuning and longer training time for optimal performance.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Model Evaluation**  \n",
    "After training, we will compare model performance using:  \n",
    "- **Mean Absolute Error (MAE)** – Measures the average difference between predicted and actual values.  \n",
    "- **R² Score** – Assesses how well the model explains variance in the data.  \n",
    "\n",
    "By testing multiple models and evaluating their performance, we will identify the most effective approach for predicting *number_of_reviews*.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b199800-6a70-4e2d-a717-144957d6bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAE: 0.8111819021485752\n",
      "Linear Regression R² Score: -0.8164122508520433\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train a Linear Regression model\n",
    "lr_model = LinearRegression()  # Create an instance of the Linear Regression model\n",
    "lr_model.fit(X_train, y_train)  # Train the model using the training dataset\n",
    "\n",
    "# Make predictions on the training and validation sets\n",
    "y_pred_train = lr_model.predict(X_train)  # Predictions on training data\n",
    "y_pred_val = lr_model.predict(X_val)  # Predictions on validation data\n",
    "\n",
    "# Evaluate the model's performance using Mean Absolute Error (MAE) and R² score\n",
    "mae = mean_absolute_error(y_val, y_pred_val)  # Calculate MAE to measure prediction accuracy\n",
    "r2 = r2_score(y_val, y_pred_val)  # Calculate R² score to evaluate model fit\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Linear Regression MAE: {mae}\")  # Lower MAE indicates better accuracy\n",
    "print(f\"Linear Regression R² Score: {r2}\")  # R² closer to 1 indicates a better fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8c32b0c-ff33-4b46-b96b-4a3cf5e2431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MAE: 0.43948764752532526\n",
      "Random Forest R² Score: 0.178419419575495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)  \n",
    "# n_estimators=100 -> The model will use 100 decision trees\n",
    "# random_state=42 -> Ensures reproducibility of results\n",
    "\n",
    "# Train the model using the training dataset\n",
    "rf_model.fit(X_train, y_train)  \n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_val = rf_model.predict(X_val)  \n",
    "\n",
    "# Evaluate model performance using Mean Absolute Error (MAE) and R² score\n",
    "mae = mean_absolute_error(y_val, y_pred_val)  # Measures the average prediction error\n",
    "r2 = r2_score(y_val, y_pred_val)  # Determines how well the model explains the variance in data\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Random Forest MAE: {mae}\")  # Lower MAE indicates better prediction accuracy\n",
    "print(f\"Random Forest R² Score: {r2}\")  # R² closer to 1 means better model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cdd64cb-ff54-43da-aa69-5ea5255c574f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 0.5089927149215583\n",
      "XGBoost R² Score: -0.05978837726929731\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor  # Import the XGBoost Regressor model\n",
    "\n",
    "# Initialize the XGBoost Regressor model\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,       # Number of boosting rounds (trees)\n",
    "    learning_rate=0.1,      # Step size shrinkage to prevent overfitting\n",
    "    random_state=42         # Ensures reproducibility of results\n",
    ")\n",
    "\n",
    "# Train the model using the training dataset\n",
    "xgb_model.fit(X_train, y_train)  \n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_val = xgb_model.predict(X_val)  \n",
    "\n",
    "# Evaluate model performance using Mean Absolute Error (MAE) and R² score\n",
    "mae = mean_absolute_error(y_val, y_pred_val)  # Measures the average prediction error\n",
    "r2 = r2_score(y_val, y_pred_val)  # Determines how well the model explains the variance in data\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"XGBoost MAE: {mae}\")  # Lower MAE indicates better prediction accuracy\n",
    "print(f\"XGBoost R² Score: {r2}\")  # R² closer to 1 means better model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d5e826e-3702-4b67-a2ac-77e745e02e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 0.5089927149215583\n",
      "XGBoost R² Score: -0.05978837726929731\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Train model\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictionsfrom xgboost import XGBRegressor  # Import the XGBoost Regressor model\n",
    "\n",
    "# Initialize the XGBoost Regressor model with key parameters\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,      # Number of trees (boosting rounds)\n",
    "    learning_rate=0.1,     # Controls how much the model adjusts with each step\n",
    "    random_state=42        # Ensures reproducibility of results\n",
    ")\n",
    "\n",
    "# Train the model using the training dataset\n",
    "xgb_model.fit(X_train, y_train)  \n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_val = xgb_model.predict(X_val)  \n",
    "\n",
    "# Evaluate model performance using Mean Absolute Error (MAE) and R² score\n",
    "mae = mean_absolute_error(y_val, y_pred_val)  # Measures the average absolute difference between actual and predicted values\n",
    "r2 = r2_score(y_val, y_pred_val)  # Determines how well the model explains the variance in data\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"XGBoost MAE: {mae}\")  # Lower MAE indicates better prediction accuracy\n",
    "print(f\"XGBoost R² Score: {r2}\")  # R² closer to 1 means better model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7a5776b-63db-4dbd-97ad-3df8c5193641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading live data\n",
    "live_data = pd.read_csv(\"listings_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27eb170d-359a-4154-8ca8-467a7d6f23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_live_data(df):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the live data to match the format of the trained model.\n",
    "    This function handles missing values, encodes categorical variables, and normalizes numerical columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Handle missing values by filling them with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Step 2: Convert categorical variables to numerical values using predefined mappings\n",
    "    # Here, \"room_type\" is mapped to numerical values for model compatibility\n",
    "    df[\"room_type\"] = df[\"room_type\"].map({\"Entire home/apt\": 0, \"Private room\": 1, \"Shared room\": 2})\n",
    "\n",
    "    # Step 3: Normalize numerical features to ensure consistent scale\n",
    "    # Standardizing the \"price\" column by subtracting the mean and dividing by the standard deviation\n",
    "    df[\"price\"] = (df[\"price\"] - df[\"price\"].mean()) / df[\"price\"].std()\n",
    "\n",
    "    return df  # Return the cleaned and transformed DataFrame\n",
    "\n",
    "# Apply preprocessing to the incoming live data\n",
    "live_data_processed = preprocess_live_data(live_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "848778de-54d5-435d-8f0f-17eec7611755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best-trained model\n",
    "import joblib\n",
    "best_model = joblib.load(\"optimized_xgb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80414213-b023-4e47-8335-a7ad31cd6bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is trained or loaded\n",
    "if \"optimized_xgb\" not in locals():\n",
    "    optimized_xgb = joblib.load(\"optimized_xgb.pkl\")  # Load the trained model if not defined\n",
    "\n",
    "# Retrieve feature names correctly\n",
    "trained_feature_names = optimized_xgb.get_booster().feature_names if hasattr(optimized_xgb, \"get_booster\") else optimized_xgb.feature_names_in_\n",
    "\n",
    "# One-Hot Encode Categorical Variables in live data\n",
    "live_data_processed = pd.get_dummies(live_data, columns=[\"room_type\", \"neighbourhood\"])\n",
    "\n",
    "# Ensure all necessary features exist in live_data_processed\n",
    "missing_cols = set(trained_feature_names) - set(live_data_processed.columns)\n",
    "for col in missing_cols:\n",
    "    live_data_processed[col] = 0  # Assign 0 to missing columns\n",
    "\n",
    "# Reorder columns to match trained model\n",
    "live_data_processed = live_data_processed[trained_feature_names]\n",
    "\n",
    "# Ensure best_model is correctly assigned (use optimized_xgb)\n",
    "best_model = optimized_xgb  # Assign the trained model\n",
    "\n",
    "# Generate predictions\n",
    "live_data[\"predicted_reviews\"] = best_model.predict(live_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f22eeb0-d2f8-4e1f-bee7-ca8b6272026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  predicted_reviews\n",
      "0  40182247          -0.241450\n",
      "1  45438479          -0.205501\n",
      "2  39793384           0.996668\n",
      "3  35942729           1.036146\n",
      "4   1468342           0.991411\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file containing predictions\n",
    "df = pd.read_csv(\"live_data_predictions.csv\")  \n",
    "\n",
    "# Display the first few rows of the dataset to verify the data\n",
    "print(df.head())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd1e8c-5bf8-470f-a057-e84522272e62",
   "metadata": {},
   "source": [
    "# **Key Findings from the Analysis**  \n",
    "\n",
    "- **Neighborhood and Room Type Influence Reviews**  \n",
    "  - Listings in specific neighborhoods and room types consistently received higher review counts, supporting our initial hypothesis.  \n",
    "\n",
    "- **Model Performance and Limitations**  \n",
    "  - The model effectively captured overall trends but struggled with extreme values, potentially due to outliers or unaccounted factors affecting review frequency.  \n",
    "\n",
    "- **Impact of Pricing on Reviews**  \n",
    "  - Price exhibited a moderate correlation with review counts, indicating that affordability may play a role in booking frequency and subsequent guest feedback.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2557f-9631-4a76-b041-fac3afc75f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
